{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Struct Vs Map Vs Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"datatype\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StructType\n",
    "- Like a mini table\n",
    "- Requires fixed schema - each row has same set of fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Person\", StructType([\n",
    "        StructField(\"Name\", StringType(), True),\n",
    "        StructField(\"Age\", IntegerType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"City\", StringType(), True)\n",
    "])\n",
    "\n",
    "data = [({\"Name\": \"Alice\", \"Age\": 30}, \"New York\"),\n",
    "        ({\"Name\": \"Bob\", \"Age\": 25}, \"Los Angeles\")]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|Person     |City       |\n",
      "+-----------+-----------+\n",
      "|{Alice, 30}|New York   |\n",
      "|{Bob, 25}  |Los Angeles|\n",
      "+-----------+-----------+\n",
      "\n",
      "root\n",
      " |-- Person: struct (nullable = true)\n",
      " |    |-- Name: string (nullable = true)\n",
      " |    |-- Age: integer (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----------+\n",
      "| Name|Age|       City|\n",
      "+-----+---+-----------+\n",
      "|Alice| 30|   New York|\n",
      "|  Bob| 25|Los Angeles|\n",
      "+-----+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Accessing the nested fields\n",
    "df.select(\"Person.Name\", \"Person.Age\", \"City\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----------+\n",
      "| Name|Age|       City|\n",
      "+-----+---+-----------+\n",
      "|Alice| 30|   New York|\n",
      "|  Bob| 25|Los Angeles|\n",
      "+-----+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"Person.Name\").alias(\"Name\"), col(\"Person.Age\").alias(\"Age\"), \"City\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----------+\n",
      "| Name|Age|       City|\n",
      "+-----+---+-----------+\n",
      "|Alice| 30|   New York|\n",
      "|  Bob| 25|Los Angeles|\n",
      "+-----+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"Person\").getItem('Name').alias(\"Name\"), col(\"Person\").getItem('Age').alias(\"Age\"), \"City\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if struct fields can vary across rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "import datetime\n",
    "schema = StructType([\n",
    "    StructField(\"Person\", StructType([\n",
    "        StructField(\"Name\", StringType(), True),\n",
    "        StructField(\"Age\", IntegerType(), True),\n",
    "        StructField(\"Birthdate\",DateType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"City\", StringType(), True)\n",
    "])\n",
    "\n",
    "data = [({\"Name\": \"Alice\", \"Age\": 30,\"Birthdate\": datetime.date(1994, 5, 10)}, \"New York\"),\n",
    "        ({\"Name\": \"Bob\", \"Age\": 25}, \"Los Angeles\")]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|              Person|       City|\n",
      "+--------------------+-----------+\n",
      "|{Alice, 30, 1994-...|   New York|\n",
      "|     {Bob, 25, NULL}|Los Angeles|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------------------+\n",
      "|Name |Attributes                                     |\n",
      "+-----+-----------------------------------------------+\n",
      "|Alice|{weight -> 60, height -> 5.5}                  |\n",
      "|Bob  |{weight -> 70, hobby -> cycling, height -> 6.0}|\n",
      "+-----+-----------------------------------------------+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Attributes: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import MapType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Attributes\", MapType(StringType(), StringType()), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (\"Alice\", {\"height\": \"5.5\", \"weight\": \"60\"}),\n",
    "    (\"Bob\", {\"height\": \"6.0\", \"weight\": \"70\", \"hobby\": \"cycling\"})\n",
    "]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+\n",
      "| Name|Height|Weight|\n",
      "+-----+------+------+\n",
      "|Alice|   5.5|    60|\n",
      "|  Bob|   6.0|    70|\n",
      "+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Accessing the map values\n",
    "df.select(\"Name\", col(\"Attributes.height\").alias(\"Height\"), col(\"Attributes.weight\").alias(\"Weight\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+\n",
      "| Name|Height|Weight|\n",
      "+-----+------+------+\n",
      "|Alice|   5.5|    60|\n",
      "|  Bob|   6.0|    70|\n",
      "+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.select(\"Name\", col(\"Attributes\").getItem(\"height\").alias(\"Height\"), col(\"Attributes\").getItem(\"weight\").alias(\"Weight\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Name |Scores      |\n",
      "+-----+------------+\n",
      "|Alice|[85, 90, 88]|\n",
      "|Bob  |[78, 82, 84]|\n",
      "+-----+------------+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Scores: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import ArrayType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Scores\", ArrayType(IntegerType()), True)\n",
    "])\n",
    "\n",
    "\n",
    "data = [\n",
    "    (\"Alice\", [85, 90, 88]),\n",
    "    (\"Bob\", [78, 82, 84])\n",
    "]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----------+\n",
      "| Name|FirstScore|SecondScore|\n",
      "+-----+----------+-----------+\n",
      "|Alice|        85|         90|\n",
      "|  Bob|        78|         82|\n",
      "+-----+----------+-----------+\n",
      "\n",
      "+-----+-----+\n",
      "| Name|Score|\n",
      "+-----+-----+\n",
      "|Alice|   85|\n",
      "|Alice|   90|\n",
      "|Alice|   88|\n",
      "|  Bob|   78|\n",
      "|  Bob|   82|\n",
      "|  Bob|   84|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Accessing array elements\n",
    "df.select(\"Name\", col(\"Scores\")[0].alias(\"FirstScore\"), col(\"Scores\")[1].alias(\"SecondScore\")).show()\n",
    "\n",
    "# Explode the array to access individual elements\n",
    "df.select(\"Name\", explode(\"Scores\").alias(\"Score\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Name |Scores      |\n",
      "+-----+------------+\n",
      "|Alice|[85, 90, 88]|\n",
      "|Bob  |[78, 82]    |\n",
      "+-----+------------+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Scores: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import ArrayType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Scores\", ArrayType(IntegerType()), True)\n",
    "])\n",
    "\n",
    "\n",
    "data = [\n",
    "    (\"Alice\", [85, 90, 88]),\n",
    "    (\"Bob\", [78, 82])\n",
    "]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
