{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Bucketing\n",
    "Reference: https://www.youtube.com/watch?v=1kWl6d1yeKA&t=1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deciding optimal number of buckets\n",
    "- size of dataset = x\n",
    "- Optimal bucket size = 128 - 200 MB\n",
    "- No. of buckets = size of dataset/optimal bucket size = 1000 MB/200 MB = 5\n",
    "\n",
    "Estimating size of dataset\n",
    "- No of megabytes  = N*V*W/1024^2\n",
    "- N = number of records\n",
    "- V = number of variables\n",
    "- W = average width in bytes of a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"bucketing\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+--------+-------------------+------------+\n",
      "|order_id|product_id|customer_id|quantity|order_date         |total_amount|\n",
      "+--------+----------+-----------+--------+-------------------+------------+\n",
      "|1       |80        |10         |4       |2023-03-20 00:00:00|1003        |\n",
      "|2       |69        |30         |3       |2023-12-11 00:00:00|780         |\n",
      "|3       |61        |20         |4       |2023-04-26 00:00:00|1218        |\n",
      "|4       |62        |44         |3       |2023-08-26 00:00:00|2022        |\n",
      "|5       |78        |46         |4       |2023-08-05 00:00:00|1291        |\n",
      "|6       |57        |34         |1       |2023-09-12 00:00:00|1529        |\n",
      "|7       |10        |24         |3       |2023-02-26 00:00:00|191         |\n",
      "|8       |46        |48         |4       |2023-10-15 00:00:00|2170        |\n",
      "|9       |57        |10         |4       |2023-01-11 00:00:00|1816        |\n",
      "|10      |31        |39         |1       |2023-01-06 00:00:00|170         |\n",
      "+--------+----------+-----------+--------+-------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df = spark.read.csv(\"../data/bucketing/orders.csv\", header=True, inferSchema=True)\n",
    "orders_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-------+-----+-----+\n",
      "|product_id|product_name|category   |brand  |price|stock|\n",
      "+----------+------------+-----------+-------+-----+-----+\n",
      "|1         |Product_1   |Electronics|Brand_4|26   |505  |\n",
      "|2         |Product_2   |Apparel    |Brand_4|489  |15   |\n",
      "|3         |Product_3   |Apparel    |Brand_4|102  |370  |\n",
      "|4         |Product_4   |Groceries  |Brand_1|47   |433  |\n",
      "|5         |Product_5   |Groceries  |Brand_3|244  |902  |\n",
      "|6         |Product_6   |Apparel    |Brand_1|268  |771  |\n",
      "|7         |Product_7   |Electronics|Brand_5|300  |229  |\n",
      "|8         |Product_8   |Groceries  |Brand_1|414  |810  |\n",
      "|9         |Product_9   |Groceries  |Brand_1|415  |224  |\n",
      "|10        |Product_10  |Electronics|Brand_5|10   |654  |\n",
      "+----------+------------+-----------+-------+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df = spark.read.csv(\"../data/bucketing/products.csv\", header=True, inferSchema=True)\n",
    "products_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_df.select('product_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_df.select('order_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join before bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [product_id#187, order_id#186, customer_id#188, quantity#189, order_date#190, total_amount#191, product_name#248, category#249, brand#250, price#251, stock#252]\n",
      "   +- SortMergeJoin [product_id#187], [product_id#247], Inner\n",
      "      :- Sort [product_id#187 ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(product_id#187, 200), ENSURE_REQUIREMENTS, [plan_id=493]\n",
      "      :     +- Filter isnotnull(product_id#187)\n",
      "      :        +- FileScan csv [order_id#186,product_id#187,customer_id#188,quantity#189,order_date#190,total_amount#191] Batched: false, DataFilters: [isnotnull(product_id#187)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/saitejadevalla/Desktop/SaiTeja/GitHub_Repos/All_things_Spa..., PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<order_id:int,product_id:int,customer_id:int,quantity:int,order_date:timestamp,total_amount...\n",
      "      +- Sort [product_id#247 ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(product_id#247, 200), ENSURE_REQUIREMENTS, [plan_id=494]\n",
      "            +- Filter isnotnull(product_id#247)\n",
      "               +- FileScan csv [product_id#247,product_name#248,category#249,brand#250,price#251,stock#252] Batched: false, DataFilters: [isnotnull(product_id#247)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/saitejadevalla/Desktop/SaiTeja/GitHub_Repos/All_things_Spa..., PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<product_id:int,product_name:string,category:string,brand:string,price:int,stock:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "po_join_df = orders_df.join(products_df, on='product_id',how='inner')\n",
    "po_join_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bucketing dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "products_df.write.bucketBy(4, col=\"product_id\").mode(\"overwrite\").saveAsTable(\"products_bucketed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.write.bucketBy(4, col=\"product_id\").mode(\"overwrite\").saveAsTable(\"orders_bucketed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [product_id#350, order_id#349, customer_id#351, quantity#352, order_date#353, total_amount#354, product_name#338, category#339, brand#340, price#341, stock#342]\n",
      "   +- SortMergeJoin [product_id#350], [product_id#337], Inner\n",
      "      :- Sort [product_id#350 ASC NULLS FIRST], false, 0\n",
      "      :  +- Filter isnotnull(product_id#350)\n",
      "      :     +- FileScan parquet spark_catalog.default.orders_bucketed[order_id#349,product_id#350,customer_id#351,quantity#352,order_date#353,total_amount#354] Batched: true, Bucketed: true, DataFilters: [isnotnull(product_id#350)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/saitejadevalla/Desktop/SaiTeja/GitHub_Repos/All_things_Spa..., PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<order_id:int,product_id:int,customer_id:int,quantity:int,order_date:timestamp,total_amount..., SelectedBucketsCount: 4 out of 4\n",
      "      +- Sort [product_id#337 ASC NULLS FIRST], false, 0\n",
      "         +- Filter isnotnull(product_id#337)\n",
      "            +- FileScan parquet spark_catalog.default.products_bucketed[product_id#337,product_name#338,category#339,brand#340,price#341,stock#342] Batched: true, Bucketed: true, DataFilters: [isnotnull(product_id#337)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/saitejadevalla/Desktop/SaiTeja/GitHub_Repos/All_things_Spa..., PartitionFilters: [], PushedFilters: [IsNotNull(product_id)], ReadSchema: struct<product_id:int,product_name:string,category:string,brand:string,price:int,stock:int>, SelectedBucketsCount: 4 out of 4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_df_bucketed = spark.table('products_bucketed')\n",
    "orders_df_bucketed = spark.table('orders_bucketed')\n",
    "\n",
    "bucketed_po_join_df = orders_df_bucketed.join(products_df_bucketed, on='product_id',how='inner')\n",
    "bucketed_po_join_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No exchange hashpartitioning after bucketing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bucketing in Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[product_id#187], functions=[sum(total_amount#191)])\n",
      "   +- Exchange hashpartitioning(product_id#187, 200), ENSURE_REQUIREMENTS, [plan_id=589]\n",
      "      +- HashAggregate(keys=[product_id#187], functions=[partial_sum(total_amount#191)])\n",
      "         +- FileScan csv [product_id#187,total_amount#191] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/saitejadevalla/Desktop/SaiTeja/GitHub_Repos/All_things_Spa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:int,total_amount:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Without bucketing\n",
    "\n",
    "sales_df = orders_df.groupBy(\"product_id\").agg(sum(\"total_amount\").alias(\"sales\"))\n",
    "\n",
    "sales_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[product_id#350], functions=[sum(total_amount#354)])\n",
      "   +- HashAggregate(keys=[product_id#350], functions=[partial_sum(total_amount#354)])\n",
      "      +- FileScan parquet spark_catalog.default.orders_bucketed[product_id#350,total_amount#354] Batched: true, Bucketed: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/saitejadevalla/Desktop/SaiTeja/GitHub_Repos/All_things_Spa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<product_id:int,total_amount:int>, SelectedBucketsCount: 4 out of 4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# With bucketing\n",
    "\n",
    "sales_df = orders_df_bucketed.groupBy(\"product_id\").agg(sum(\"total_amount\").alias(\"sales\"))\n",
    "\n",
    "sales_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No exchange hashpartitioning after bucketing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bucket pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[product_id#350], functions=[sum(total_amount#354)])\n",
      "   +- HashAggregate(keys=[product_id#350], functions=[partial_sum(total_amount#354)])\n",
      "      +- Filter (isnotnull(product_id#350) AND (product_id#350 = 1))\n",
      "         +- FileScan parquet spark_catalog.default.orders_bucketed[product_id#350,total_amount#354] Batched: true, Bucketed: true, DataFilters: [isnotnull(product_id#350), (product_id#350 = 1)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/saitejadevalla/Desktop/SaiTeja/GitHub_Repos/All_things_Spa..., PartitionFilters: [], PushedFilters: [IsNotNull(product_id), EqualTo(product_id,1)], ReadSchema: struct<product_id:int,total_amount:int>, SelectedBucketsCount: 1 out of 4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_sales_bucket_pruning = (\n",
    "    orders_df_bucketed\n",
    "    .filter(col(\"product_id\") == 1)\n",
    "    .groupBy(\"product_id\")\n",
    "    .agg(sum(\"total_amount\").alias(\"sales\"))\n",
    ")\n",
    "product_sales_bucket_pruning.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
